{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning on a pre-trained Wav2Vec2 on small datasets for Urdu-ASR:\n",
    "\n",
    "This notebook explores the practicality of transfer learning (fine-tuning) for Automatic Speech Recognition. Our code mostly follows: [Fine-tune Wav2vec for English ASR](https://huggingface.co/blog/fine-tune-wav2vec2-english) by Patrick von Platen. Details on the Wav2Vec2 model can be found [here](https://ai.meta.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/). \n",
    "\n",
    "The [Wav2Vec2 model](https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec#wav2vec-20) used was pre-trained on the Librispeech corpus.\n",
    "\n",
    "In addition, we wanted to explore the usefulness of pre-trained models on resource-starved environments. In particular we wanted to test the following:\n",
    "- The feasilibity of low end PCs for this task. Our machine had the GTX 1050M as the GPU with 4GB of available VRAM. This is a low-end GPU available with many low-end and affordable laptops. However, we had to adapt our code to be able to run locally and on a low-end device.\n",
    "\n",
    "- The practicality of fine-tuning the model on different languages. For this, we fine-tuned the model on publicly availble datasets in Urdu.\n",
    "\n",
    "- The possibility of expansion to different languages. While English and Urdu are vastly different languages, the north-western and the northern regions of the sub-continent present dialect continuums locally and areally. The distinction between language, dialect and accent is often fuzzy. While prestige dialects and languages have enough available resources to have state-of-the-art ASR, dialects with lower prestige do not. Future work could explore the effectiveness of ASR models trained on prestige dialects in the aformentioned regions on dialects with less prestige. For this English and Urdu, while not being entirely analogous, serve as a stand-in.\n",
    "\n",
    "\n",
    "### Summarizing the Methodology and Results:\n",
    "- Our methodology uses gradient accumulation and a batch size of 1 to allow for the model to run on such a low-end device.\n",
    "\n",
    "- We obtained a WER of 0.225. Given our small dataset, this is a very good result. For comparison, state-of-the-art ASR models tend to have a WER of 0.150-0.200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import random\n",
    "import pyarrow as pa\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2ForCTC, Trainer\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import transformers\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\" # Allow transformers from huggingface to run offline.\n",
    "num_speakers = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading and Preprocessing:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset:\n",
    "Our dataset is composed of the recordings of the PRUS dataset and a compilation of news clippings. The speakers are undergraduates from LUMS.\n",
    "\n",
    "- [The PRUS dataset](https://www.c-salt.org/downloads/prus): A compilation of 708 urdu sentences generated using a greedy approach. The goal was to obtain a dataset with all tri-phoneme combination (word-boundaries included) possible within Urdu, with each tri-phoneme equally likely. The dataset consists of naturally occuring words in Urdu, and is grammatically correct. However, the dataset is not representative of natural Urdu speech and the syntax, word-order, sentence length and the lexicon of many of the sentences is rarely or never part of natural Urdu speech. \n",
    "\n",
    "- Newspaper clippings: A compilation of 505 headlines and sentences from various news articles and newspapers within the last few years. Compared with the PRUS dataset, this dataset is far more representative of modern, albeit highly formal Urdu speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transcriptions and preprocessing:\n",
    "\n",
    "- The vocabulary consists of the Urdu alphabet plus a few other characters deemed necessary.\n",
    "\n",
    "- The data has been pre-processed to remove any characters that do not represent a phoneme.\n",
    "\n",
    "- Harakat (used to demarcate vowels) are not considered part of the vocabulary given that the Urdu script is a partial Abjad and the reader is expected to infer them.\n",
    "\n",
    "- The tashdeed (used to represent gemination) is not considered part of the vocabulary given that gemination follows specific rules and is readily infered from the context.\n",
    "\n",
    "- Digits are replaced with their word representations to facilitate the training of the model. This also means that for any new speech, any numbers spoken will be represented using words.\n",
    "\n",
    "- In addition, a few special unicode characters are removed (\\u200c and \\u200e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prus_transcriptions = open(\"./Dataset/Trasncriptions - PRUS.txt\" , 'r', encoding = \"UTF-8\")\n",
    "prus_transcriptions = prus_transcriptions.readlines()\n",
    "\n",
    "news_transcriptions = open(\"./Dataset/Transcriptions - News clippings.txt\", 'r', encoding = \"UTF-8\")\n",
    "news_transcriptions = news_transcriptions.readlines()\n",
    "\n",
    "to_ignore_regex = \"[\\,\\?\\.\\!\\-\\;\\:\\\"\\\\()'؟۔’‘،ًٌَُِّٰٔٓ]\"\n",
    "\n",
    "# PRUS Dataset\n",
    "for i in range(len(prus_transcriptions)):\n",
    "    prus_transcriptions[i] = prus_transcriptions[i].replace(\"\\n\",\"\")\n",
    "    prus_transcriptions[i] = re.sub(to_ignore_regex, \"\", prus_transcriptions[i])\n",
    "    prus_transcriptions[i] = re.sub(\"0\", \"صفر\", prus_transcriptions[i])\n",
    "    prus_transcriptions[i] = re.sub(\"1\", \"ایک\", prus_transcriptions[i])\n",
    "    prus_transcriptions[i] = re.sub(\"2\", \"دو\", prus_transcriptions[i])\n",
    "    prus_transcriptions[i] = re.sub(\"3\", \"تین\", prus_transcriptions[i])\n",
    "    prus_transcriptions[i] = re.sub(\"4\", \"چار\", prus_transcriptions[i])\n",
    "    prus_transcriptions[i] = re.sub(\"5\", \"پانچ\", prus_transcriptions[i])\n",
    "    prus_transcriptions[i] = re.sub(\"6\", \"چھ\", prus_transcriptions[i])\n",
    "    prus_transcriptions[i] = re.sub(\"7\", \"سات\", prus_transcriptions[i])\n",
    "    prus_transcriptions[i] = re.sub(\"8\", \"اٹھ\", prus_transcriptions[i])\n",
    "    prus_transcriptions[i] = re.sub(\"9\", \"نو\", prus_transcriptions[i])\n",
    "    prus_transcriptions[i] = re.sub(\"\\u200c\", \"\", prus_transcriptions[i])\n",
    "    prus_transcriptions[i] = re.sub(\"\\u200e\", \"\", prus_transcriptions[i])\n",
    "\n",
    "# News Clippings Dataset\n",
    "for i in range(len(news_transcriptions)):\n",
    "    news_transcriptions[i] = news_transcriptions[i].replace(\"\\n\",\"\")\n",
    "    news_transcriptions[i] = re.sub(to_ignore_regex, \"\", news_transcriptions[i])\n",
    "    news_transcriptions[i] = re.sub(\"0\", \"صفر\", news_transcriptions[i])\n",
    "    news_transcriptions[i] = re.sub(\"1\", \"ایک\", news_transcriptions[i])\n",
    "    news_transcriptions[i] = re.sub(\"2\", \"دو\", news_transcriptions[i])\n",
    "    news_transcriptions[i] = re.sub(\"3\", \"تین\", news_transcriptions[i])\n",
    "    news_transcriptions[i] = re.sub(\"4\", \"چار\", news_transcriptions[i])\n",
    "    news_transcriptions[i] = re.sub(\"5\", \"پانچ\", news_transcriptions[i])\n",
    "    news_transcriptions[i] = re.sub(\"6\", \"چھ\", news_transcriptions[i])\n",
    "    news_transcriptions[i] = re.sub(\"7\", \"سات\", news_transcriptions[i])\n",
    "    news_transcriptions[i] = re.sub(\"8\", \"اٹھ\", news_transcriptions[i])\n",
    "    news_transcriptions[i] = re.sub(\"9\", \"نو\", news_transcriptions[i])\n",
    "    news_transcriptions[i] = re.sub(\"\\u200c\", \"\", news_transcriptions[i])\n",
    "    news_transcriptions[i] = re.sub(\"\\u200e\", \"\", news_transcriptions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Audio data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prus_path = \"./Dataset/PRUS/\"\n",
    "news_path = \"./Dataset/News_Clippings/\"\n",
    "\n",
    "# files = []\n",
    "audios = []\n",
    "transcriptions = []\n",
    "\n",
    "# PRUS dataset\n",
    "for i in range(num_speakers):\n",
    "    for j in range(len(prus_transcriptions)):\n",
    "        # File\n",
    "        file_path = prus_path + str(j+1) + \"_\" + \"speaker\" + str(i+1) + \"_prus.wav\"\n",
    "        # files.append(file_path)\n",
    "        \n",
    "        # Audio\n",
    "        array, sampling_rate = librosa.load(file_path, sr=16000)\n",
    "        audios.append({\"array\": array, \"path\": file_path, \"sampling_rate\": sampling_rate})\n",
    "\n",
    "        # Transcriptions\n",
    "        transcriptions.append(prus_transcriptions[j])\n",
    "\n",
    "# News dataset\n",
    "for i in range(num_speakers):\n",
    "    for j in range(len(news_transcriptions)):\n",
    "        # File\n",
    "        file_path = news_path + str(j+1) + \"_\" + \"speaker\" + str(i+1) + \"_news.wav\"\n",
    "        # files.append(file_path)\n",
    "        \n",
    "        # Audio\n",
    "        array, sampling_rate = librosa.load(file_path, sr=16000)\n",
    "        audios.append({\"array\": array, \"path\": file_path, \"sampling_rate\": sampling_rate})\n",
    "\n",
    "        # Transcriptions\n",
    "        transcriptions.append(news_transcriptions[j])\n",
    "\n",
    "\n",
    "# Randomizing\n",
    "# randomized_data = list(zip(files, audios, transcriptions))\n",
    "randomized_data = list(zip(audios, transcriptions))\n",
    "random.shuffle(randomized_data)\n",
    "\n",
    "# files, audios, transcriptions = zip(*randomized_data)\n",
    "audios, transcriptions = zip(*randomized_data)\n",
    "\n",
    "randomized_data = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split:\n",
    "\n",
    "The model is trained on 80% of the data and evaluated on the remaining 20%. The data is randomized before splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(audios)\n",
    "train_split = 0.8\n",
    "train_size = int(train_split * dataset_size)\n",
    "\n",
    "train_files = []\n",
    "train_audios = []\n",
    "train_transcriptions = []\n",
    "\n",
    "test_files = []\n",
    "test_audios = []\n",
    "test_transcriptions = []\n",
    "\n",
    "# Training data\n",
    "for i in range(0, train_size):\n",
    "    #train_files.append(files[i])\n",
    "    train_audios.append(audios[i])\n",
    "    train_transcriptions.append(transcriptions[i])\n",
    "\n",
    "# Test data\n",
    "for i in range(train_size, dataset_size):\n",
    "    #test_files.append(files[i])\n",
    "    test_audios.append(audios[i])\n",
    "    test_transcriptions.append(transcriptions[i])\n",
    "\n",
    "\n",
    "# Creating the train and test datasets.\n",
    "# df = pd.DataFrame({'file': train_files, 'audio': train_audios , 'text' : train_transcriptions})\n",
    "df = pd.DataFrame({'audio': train_audios , 'transcription' : train_transcriptions})\n",
    "train_dataset = Dataset(pa.Table.from_pandas(df))\n",
    "\n",
    "# df = pd.DataFrame({'file': test_files, 'audio': test_audios , 'text' : test_transcriptions})\n",
    "df = pd.DataFrame({'audio': test_audios , 'transcription' : test_transcriptions})\n",
    "test_dataset = Dataset(pa.Table.from_pandas(df))\n",
    "\n",
    "# Creating the overall urdu dataset used for fine-tuning the model.\n",
    "ur_fine_dataset = DatasetDict({\"train\": train_dataset , \"test\": test_dataset})\n",
    "ur_fine_dataset\n",
    "\n",
    "train_dataset = 0\n",
    "test_dataset = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training dataset transcriptions after pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayTranscriptions(dataset, samples=5):\n",
    "    # Warning\n",
    "    if samples >= len(dataset):\n",
    "        print(\"Warning: can only display a maximum of\", len(dataset), \"samples\")\n",
    "        samples = len(dataset)\n",
    "    # Creating indicies\n",
    "    indices = []\n",
    "    for _ in range(samples):\n",
    "        i = random.randint(0, len(dataset)-1)\n",
    "        \n",
    "        while i in indices:\n",
    "            i = random.randint(0, len(dataset)-1)\n",
    "        \n",
    "        indices.append(i)\n",
    "    \n",
    "    dataframe = pd.DataFrame(dataset[indices][\"transcription\"])\n",
    "    dataframe.columns = [\"Transcriptions\"]\n",
    "    display(HTML(dataframe.to_html())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transcriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>خیال کیا جاتا ہے کہ اس طرح کی جاسوسی کا زیادہ تر نشانہ سماجی کارکن صحافی اور سیاستدان بنتے ہیں</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>بدصورت بانسری کی انوالومنٹ سے سیلرز کی ہیرو سے گراں تخریب نیچرل تھی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>حروں نے شہابیے کو ساگر میں پھینکا اور پیداوار کے رزق کی ڈیل کرنے کے بعد گاوں سے اوجھل ہو گئے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>سوم درجے کی جعلسازی کا بھیڑیا تھیلی اٹھایا اور اپنے پڑدادا کے جلو میں دب گیا ہے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>غیر جانبدار افغان بھابی کے ساتھ نتھی گڑیا کا موڈ بدلیں تو خفت رفو ہو سکتی ہے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>میلے کچیلے خام فوم کے سویٹر میں کلراٹھی چقندر باندھیں تو یہی شے خوبصورت ڈھال ہے</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>وائس آف امریکہ سے بات کرتے ہوئے ان کا کہنا تھا کہ وہ ڈرامے کے اختتام سے بالکل خوش نہیں</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>گاڑی کے نظام میں ایسی کوئی چیز نہیں ہے جو یہ تصدیق کر سکے کہ ویڈیو گیم کھیلنے والا ڈرائیور ہے یا اس کے ساتھ والی سیٹ پر بیٹھا ہوا مسافر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>دنیا بھر میں پیٹرولیم مصنوعات کی قیمتوں میں مسلسل اضافے سے لوگ پریشان ہیں</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>گگلی کے تجربہ اور تشخیص پر ہدایت اور تعریفیں چاہتے ہیں</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayTranscriptions(ur_fine_dataset[\"train\"], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the vocabulary:\n",
    "\n",
    "The initial vocabulary consists of the Urdu alphabet plus a few other characters deemed necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['چ', 'ٹ', 'گ', 'پ', 'ء', 'خ', 'م', 'ت', 'ص', 'ڑ', 'ق', 'ژ', 'ں', 'ئ', 'ر', 'ع', 'ے', 'ط', 'ا', 'ڈ', 'ذ', 'ک', 'غ', 'س', 'ف', 'ج', 'ش', 'ہ', 'ن', 'ؤ', 'ب', 'ض', 'ح', 'ز', 'ظ', 'آ', 'و', 'ل', 'ی', 'ث', 'د', 'ھ', '|', '[UNK]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "def generateVocab(dataset):\n",
    "    for i in range(len(dataset)):\n",
    "        text = \" \".join(dataset[\"transcription\"])\n",
    "    \n",
    "    return list(set(text))\n",
    "\n",
    "\n",
    "vocabulary = generateVocab(ur_fine_dataset[\"train\"])\n",
    "vocabulary.append(\"|\")\n",
    "vocabulary.remove(\" \")\n",
    "vocabulary.append(\"[UNK]\")\n",
    "vocabulary.append(\"[PAD]\")\n",
    "print(vocabulary)\n",
    "\n",
    "# Creating a vocabulary dictionary\n",
    "vocabulary_dictionary = {}\n",
    "\n",
    "i = 0\n",
    "for char in vocabulary:\n",
    "    vocabulary_dictionary[char] = i\n",
    "    i += 1\n",
    "\n",
    "vocabulary_size = len(vocabulary_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to json:\n",
    "The vocabulary is extracted to a json file to allow it to be used as input to the Wav2Vec2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocabulary.json\", \"w\") as json_file:\n",
    "    json.dump(vocabulary_dictionary, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model preparing and training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer\n",
    "\n",
    "Initializes the tokenizer, a huggingface made class which converts the input data into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocabulary.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Model/final_model/tokenizer_config.json',\n",
       " './Model/final_model/special_tokens_map.json',\n",
       " './Model/final_model/vocab.json',\n",
       " './Model/final_model/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./Model/final_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extractor\n",
    "\n",
    "Initializes the feature extractor, a huggingface made class which prepares audio data into features to use within in a model. More information can be found [here](https://huggingface.co/docs/transformers/main_classes/feature_extractor#:~:text=A%20feature%20extractor%20is%20in,for%20audio%20or%20vision%20models.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./Model/final_model/preprocessor_config.json']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor.save_pretrained(\"./Model/final_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processor:\n",
    "\n",
    "Combines the tokenizer and feature extractor into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing the audio data for training:\n",
    "\n",
    "- The audio is loaded and resampled (not needed in our case)\n",
    "\n",
    "- The input values are stored after processing. In this case, the audio data is only normalized.\n",
    "\n",
    "- The transcriptions are encoded to label IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingAudio(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate = audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"transcription\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7559f49f80274d3fa27c8f136e1edb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2911 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10bbf09acf0420a873cd037aea86709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ur_fine_dataset = ur_fine_dataset.map(preprocessingAudio, remove_columns = ur_fine_dataset.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        input_values = batch['input_values']\n",
    "        input_values = input_values.cuda()\n",
    "\n",
    "      \n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"input_values\"] = input_values\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "# The metric used is Word Error Rate (WER) as it the most commonly used method for ASR models.\n",
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the WER on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeWER(predicted):\n",
    "    predicted_logits = predicted.predictions\n",
    "    predicted_ids = np.argmax(predicted_logits, axis=-1)\n",
    "\n",
    "    predicted.label_ids[predicted.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(predicted_ids)\n",
    "    \n",
    "    # We do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(predicted.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Wav2Vec2pretrained checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.weight', 'project_hid.bias', 'project_hid.weight', 'quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_q.bias', 'project_q.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\", \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size = vocabulary_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\".\\Model\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=1,\n",
    "  per_device_eval_batch_size=1,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=30,\n",
    "  fp16=False, # Set to True for better performance\n",
    "  gradient_checkpointing=True,\n",
    "  save_steps=50,\n",
    "  eval_steps=50,\n",
    "  logging_steps=50,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=2,\n",
    "  push_to_hub=False, # Set to false if running offline\n",
    "  dataloader_pin_memory=False,\n",
    "  gradient_accumulation_steps=30 # To reduce memory usage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=computeWER,\n",
    "    train_dataset = ur_fine_dataset[\"train\"],\n",
    "    eval_dataset = ur_fine_dataset[\"test\"],\n",
    "    tokenizer=processor.feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8358fcf31e7e472998b23e3d044bd89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.1758, 'learning_rate': 5e-06, 'epoch': 0.52}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac0904b02864773aff87c3ed633e2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.392762184143066, 'eval_wer': 1.0, 'eval_runtime': 265.0949, 'eval_samples_per_second': 2.746, 'eval_steps_per_second': 2.746, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.9365, 'learning_rate': 1e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba5c569371a40a4b3479808e88bd80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.467226028442383, 'eval_wer': 1.0, 'eval_runtime': 261.0537, 'eval_samples_per_second': 2.789, 'eval_steps_per_second': 2.789, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4088, 'learning_rate': 1.5e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63070e69c66f43428f185d3ddf2be8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.271307945251465, 'eval_wer': 1.0, 'eval_runtime': 263.1092, 'eval_samples_per_second': 2.767, 'eval_steps_per_second': 2.767, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2356, 'learning_rate': 2e-05, 'epoch': 2.06}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e51d073a48436a8ace64a0f5a8ba76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1610107421875, 'eval_wer': 1.0, 'eval_runtime': 269.128, 'eval_samples_per_second': 2.705, 'eval_steps_per_second': 2.705, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1421, 'learning_rate': 2.5e-05, 'epoch': 2.58}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b70f5a4e60c4203adc50d791da3ea9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.11612606048584, 'eval_wer': 1.0, 'eval_runtime': 269.4766, 'eval_samples_per_second': 2.702, 'eval_steps_per_second': 2.702, 'epoch': 2.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1074, 'learning_rate': 3e-05, 'epoch': 3.09}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c715a1f1107a45e29df6f42f91194c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.098893404006958, 'eval_wer': 1.0, 'eval_runtime': 270.9497, 'eval_samples_per_second': 2.687, 'eval_steps_per_second': 2.687, 'epoch': 3.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0951, 'learning_rate': 3.5e-05, 'epoch': 3.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32ecd5225ed4a3ebf5006ccd1940e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0826168060302734, 'eval_wer': 1.0, 'eval_runtime': 268.0129, 'eval_samples_per_second': 2.716, 'eval_steps_per_second': 2.716, 'epoch': 3.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0873, 'learning_rate': 4e-05, 'epoch': 4.12}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d318e7a1a644931b9319139918b5037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0749130249023438, 'eval_wer': 1.0, 'eval_runtime': 267.5487, 'eval_samples_per_second': 2.721, 'eval_steps_per_second': 2.721, 'epoch': 4.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0761, 'learning_rate': 4.5e-05, 'epoch': 4.64}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556b3398673546c8b9b76c74f5bc5950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0334696769714355, 'eval_wer': 1.0, 'eval_runtime': 267.2168, 'eval_samples_per_second': 2.724, 'eval_steps_per_second': 2.724, 'epoch': 4.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8768, 'learning_rate': 5e-05, 'epoch': 5.15}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1eea5dbb948459da2f8875300a4b6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4343087673187256, 'eval_wer': 0.9988654413433175, 'eval_runtime': 267.0482, 'eval_samples_per_second': 2.726, 'eval_steps_per_second': 2.726, 'epoch': 5.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0997, 'learning_rate': 5.500000000000001e-05, 'epoch': 5.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415b7f65160d49b1a54ddcab0abcd8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5502562522888184, 'eval_wer': 0.8225550260948491, 'eval_runtime': 268.1847, 'eval_samples_per_second': 2.715, 'eval_steps_per_second': 2.715, 'epoch': 5.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4734, 'learning_rate': 6e-05, 'epoch': 6.18}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288f31d161944a82aaf56af19c23a737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1019192934036255, 'eval_wer': 0.6912487708947886, 'eval_runtime': 268.1733, 'eval_samples_per_second': 2.715, 'eval_steps_per_second': 2.715, 'epoch': 6.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1732, 'learning_rate': 6.500000000000001e-05, 'epoch': 6.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29351f3124a840b3969b4d1a258563d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9048839211463928, 'eval_wer': 0.6446562287270252, 'eval_runtime': 215.4731, 'eval_samples_per_second': 3.379, 'eval_steps_per_second': 3.379, 'epoch': 6.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9855, 'learning_rate': 7e-05, 'epoch': 7.21}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f656851dad304dafb6bfc94524ab06c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7670953869819641, 'eval_wer': 0.5870962862113305, 'eval_runtime': 153.257, 'eval_samples_per_second': 4.75, 'eval_steps_per_second': 4.75, 'epoch': 7.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8512, 'learning_rate': 7.500000000000001e-05, 'epoch': 7.73}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8ee72766db40bc971c8e7098663dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6952930092811584, 'eval_wer': 0.545117615914076, 'eval_runtime': 150.6908, 'eval_samples_per_second': 4.831, 'eval_steps_per_second': 4.831, 'epoch': 7.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.777, 'learning_rate': 8e-05, 'epoch': 8.24}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bec43b087142bda5e6f8881b74bcc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6270283460617065, 'eval_wer': 0.5287043340140686, 'eval_runtime': 155.7298, 'eval_samples_per_second': 4.675, 'eval_steps_per_second': 4.675, 'epoch': 8.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7094, 'learning_rate': 8.5e-05, 'epoch': 8.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34840c80bc534ac382cb0231fd07aee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5807090997695923, 'eval_wer': 0.5037440435670524, 'eval_runtime': 154.276, 'eval_samples_per_second': 4.719, 'eval_steps_per_second': 4.719, 'epoch': 8.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6405, 'learning_rate': 9e-05, 'epoch': 9.28}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d56fea246394da6821f137ac8a082c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5506355166435242, 'eval_wer': 0.4925497314877846, 'eval_runtime': 151.9131, 'eval_samples_per_second': 4.792, 'eval_steps_per_second': 4.792, 'epoch': 9.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6126, 'learning_rate': 9.5e-05, 'epoch': 9.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154e93c173c843a4b084aace5576938d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5124320983886719, 'eval_wer': 0.4785568413886998, 'eval_runtime': 150.7415, 'eval_samples_per_second': 4.829, 'eval_steps_per_second': 4.829, 'epoch': 9.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5562, 'learning_rate': 0.0001, 'epoch': 10.31}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d854318bb640e6917b28bd70e54144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47998449206352234, 'eval_wer': 0.4591937069813176, 'eval_runtime': 151.6243, 'eval_samples_per_second': 4.801, 'eval_steps_per_second': 4.801, 'epoch': 10.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5187, 'learning_rate': 9.738219895287959e-05, 'epoch': 10.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe50884452964166a60aa48ee23b5071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4567178189754486, 'eval_wer': 0.4325693971711671, 'eval_runtime': 152.7046, 'eval_samples_per_second': 4.767, 'eval_steps_per_second': 4.767, 'epoch': 10.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4883, 'learning_rate': 9.476439790575917e-05, 'epoch': 11.34}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ecd035ffb04ef4abea48f1432ff381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43991777300834656, 'eval_wer': 0.42114817336056276, 'eval_runtime': 153.5424, 'eval_samples_per_second': 4.741, 'eval_steps_per_second': 4.741, 'epoch': 11.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4674, 'learning_rate': 9.214659685863875e-05, 'epoch': 11.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8ab0f0935f4ef8a829afd46723648d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43530893325805664, 'eval_wer': 0.4116935178882081, 'eval_runtime': 153.0964, 'eval_samples_per_second': 4.755, 'eval_steps_per_second': 4.755, 'epoch': 11.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4187, 'learning_rate': 8.952879581151833e-05, 'epoch': 12.37}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6dd5f3b1f94cbd80aa32956c3ec04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4159089922904968, 'eval_wer': 0.39891082368958475, 'eval_runtime': 154.9479, 'eval_samples_per_second': 4.698, 'eval_steps_per_second': 4.698, 'epoch': 12.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4096, 'learning_rate': 8.691099476439791e-05, 'epoch': 12.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9c4e525c1849f1b3d6cfd2fb2e743f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4110824167728424, 'eval_wer': 0.3916496482868164, 'eval_runtime': 154.4314, 'eval_samples_per_second': 4.714, 'eval_steps_per_second': 4.714, 'epoch': 12.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3838, 'learning_rate': 8.429319371727749e-05, 'epoch': 13.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433bebd66e5248b093593a9b2d8c80fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38556620478630066, 'eval_wer': 0.3812873458891158, 'eval_runtime': 150.5022, 'eval_samples_per_second': 4.837, 'eval_steps_per_second': 4.837, 'epoch': 13.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3626, 'learning_rate': 8.167539267015707e-05, 'epoch': 13.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c57d0e39a74069a678bf5dea808a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39861273765563965, 'eval_wer': 0.3750850918992512, 'eval_runtime': 151.393, 'eval_samples_per_second': 4.809, 'eval_steps_per_second': 4.809, 'epoch': 13.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3464, 'learning_rate': 7.905759162303665e-05, 'epoch': 14.43}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9904c229709e4f9aa2dab103aca816e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37521669268608093, 'eval_wer': 0.3690341123969443, 'eval_runtime': 151.5491, 'eval_samples_per_second': 4.804, 'eval_steps_per_second': 4.804, 'epoch': 14.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3389, 'learning_rate': 7.643979057591623e-05, 'epoch': 14.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4b0bbed96740d791cfebcddd658c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3773742914199829, 'eval_wer': 0.36283185840707965, 'eval_runtime': 152.9948, 'eval_samples_per_second': 4.758, 'eval_steps_per_second': 4.758, 'epoch': 14.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3126, 'learning_rate': 7.382198952879581e-05, 'epoch': 15.46}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a41777945ef4dfbb99d77bf63901456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36431893706321716, 'eval_wer': 0.35746161409878224, 'eval_runtime': 156.2885, 'eval_samples_per_second': 4.658, 'eval_steps_per_second': 4.658, 'epoch': 15.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2971, 'learning_rate': 7.12041884816754e-05, 'epoch': 15.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c7d5d8cd724e328fba65058a37c2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3718809187412262, 'eval_wer': 0.3567808789047727, 'eval_runtime': 153.104, 'eval_samples_per_second': 4.755, 'eval_steps_per_second': 4.755, 'epoch': 15.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2799, 'learning_rate': 6.858638743455498e-05, 'epoch': 16.49}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea6d86855904f709fbb0519db8b369d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3621373474597931, 'eval_wer': 0.3489902427955525, 'eval_runtime': 150.5702, 'eval_samples_per_second': 4.835, 'eval_steps_per_second': 4.835, 'epoch': 16.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2785, 'learning_rate': 6.596858638743456e-05, 'epoch': 17.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c74a90e38b94d6d8dd0d25bd27b62dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3542162775993347, 'eval_wer': 0.33809847969140006, 'eval_runtime': 154.2163, 'eval_samples_per_second': 4.721, 'eval_steps_per_second': 4.721, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2475, 'learning_rate': 6.335078534031414e-05, 'epoch': 17.52}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bcd18dc8bc449fb151d792c5481c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34813857078552246, 'eval_wer': 0.3377202934725059, 'eval_runtime': 150.4849, 'eval_samples_per_second': 4.838, 'eval_steps_per_second': 4.838, 'epoch': 17.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.254, 'learning_rate': 6.073298429319372e-05, 'epoch': 18.04}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a84b16f2284dd7aa18dd5e74fe2252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3441419005393982, 'eval_wer': 0.3365100975720445, 'eval_runtime': 154.0655, 'eval_samples_per_second': 4.725, 'eval_steps_per_second': 4.725, 'epoch': 18.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2343, 'learning_rate': 5.81151832460733e-05, 'epoch': 18.55}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14549e692f824c698f772a8e1bf56629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3361930549144745, 'eval_wer': 0.3324256864079873, 'eval_runtime': 148.5492, 'eval_samples_per_second': 4.901, 'eval_steps_per_second': 4.901, 'epoch': 18.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2303, 'learning_rate': 5.5497382198952887e-05, 'epoch': 19.07}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e88462f06a141e9954cae49d8cc7bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3295890986919403, 'eval_wer': 0.321912109522729, 'eval_runtime': 153.3698, 'eval_samples_per_second': 4.747, 'eval_steps_per_second': 4.747, 'epoch': 19.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2168, 'learning_rate': 5.287958115183246e-05, 'epoch': 19.58}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf41d2662d6a46f2afa69f6cf26cc826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3416559398174286, 'eval_wer': 0.3278874517812571, 'eval_runtime': 157.1729, 'eval_samples_per_second': 4.632, 'eval_steps_per_second': 4.632, 'epoch': 19.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2198, 'learning_rate': 5.026178010471204e-05, 'epoch': 20.1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490f9f5ca2914b64a343763c666ac26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3367469906806946, 'eval_wer': 0.32229029574162316, 'eval_runtime': 151.4035, 'eval_samples_per_second': 4.808, 'eval_steps_per_second': 4.808, 'epoch': 20.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2001, 'learning_rate': 4.764397905759162e-05, 'epoch': 20.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10153c90434b47fcacfc9ee8265264e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33665335178375244, 'eval_wer': 0.30716284698585583, 'eval_runtime': 151.167, 'eval_samples_per_second': 4.816, 'eval_steps_per_second': 4.816, 'epoch': 20.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2033, 'learning_rate': 4.50261780104712e-05, 'epoch': 21.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68309a8d4d045e88ad454fd60e7f218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3217971920967102, 'eval_wer': 0.30882686634899026, 'eval_runtime': 151.1469, 'eval_samples_per_second': 4.817, 'eval_steps_per_second': 4.817, 'epoch': 21.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1923, 'learning_rate': 4.240837696335079e-05, 'epoch': 21.64}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc062b50f5de4926bbef6ed14c72a03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3307955265045166, 'eval_wer': 0.306179562816731, 'eval_runtime': 149.2358, 'eval_samples_per_second': 4.878, 'eval_steps_per_second': 4.878, 'epoch': 21.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1951, 'learning_rate': 3.9790575916230365e-05, 'epoch': 22.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5459392699463790d61ce1368e32e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33941909670829773, 'eval_wer': 0.2948339762499054, 'eval_runtime': 156.5059, 'eval_samples_per_second': 4.652, 'eval_steps_per_second': 4.652, 'epoch': 22.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1802, 'learning_rate': 3.717277486910995e-05, 'epoch': 22.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b47e918ba24a1eac2ff3ecc90ee97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31174254417419434, 'eval_wer': 0.29733000529460707, 'eval_runtime': 159.961, 'eval_samples_per_second': 4.551, 'eval_steps_per_second': 4.551, 'epoch': 22.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1837, 'learning_rate': 3.455497382198953e-05, 'epoch': 23.19}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a571d1ca6014b2581c5363d5b618bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32396313548088074, 'eval_wer': 0.2909764768171848, 'eval_runtime': 157.0471, 'eval_samples_per_second': 4.636, 'eval_steps_per_second': 4.636, 'epoch': 23.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1698, 'learning_rate': 3.1937172774869115e-05, 'epoch': 23.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e2e35cc5e54bbf9e3974423e237e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3133009076118469, 'eval_wer': 0.2884048105287043, 'eval_runtime': 160.4001, 'eval_samples_per_second': 4.539, 'eval_steps_per_second': 4.539, 'epoch': 23.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1728, 'learning_rate': 2.931937172774869e-05, 'epoch': 24.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9af4d79f0e40b6b3f6fe0068620b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30827319622039795, 'eval_wer': 0.2865138794342334, 'eval_runtime': 161.7534, 'eval_samples_per_second': 4.501, 'eval_steps_per_second': 4.501, 'epoch': 24.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1651, 'learning_rate': 2.6701570680628273e-05, 'epoch': 24.73}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06974032fc64cfb9b1ee2cf2c9d2e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3188498914241791, 'eval_wer': 0.2874971636033583, 'eval_runtime': 165.5186, 'eval_samples_per_second': 4.398, 'eval_steps_per_second': 4.398, 'epoch': 24.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1609, 'learning_rate': 2.4083769633507854e-05, 'epoch': 25.25}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a100db866c4577b4564599eab9d8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3089900612831116, 'eval_wer': 0.28182437031994556, 'eval_runtime': 161.2452, 'eval_samples_per_second': 4.515, 'eval_steps_per_second': 4.515, 'epoch': 25.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1588, 'learning_rate': 2.1465968586387435e-05, 'epoch': 25.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0980cb137dc407dbc733995f0d3e4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3090582489967346, 'eval_wer': 0.28099236063837835, 'eval_runtime': 172.1621, 'eval_samples_per_second': 4.229, 'eval_steps_per_second': 4.229, 'epoch': 25.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1525, 'learning_rate': 1.8848167539267016e-05, 'epoch': 26.28}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ee8a24897e4eafbbbaaac7a8a503a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30835938453674316, 'eval_wer': 0.27834505710611906, 'eval_runtime': 159.5166, 'eval_samples_per_second': 4.564, 'eval_steps_per_second': 4.564, 'epoch': 26.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1501, 'learning_rate': 1.6230366492146596e-05, 'epoch': 26.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bce03016334a789c8cb2a85dd1b147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31023889780044556, 'eval_wer': 0.27978216473791695, 'eval_runtime': 161.9051, 'eval_samples_per_second': 4.496, 'eval_steps_per_second': 4.496, 'epoch': 26.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1477, 'learning_rate': 1.3612565445026179e-05, 'epoch': 27.31}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f491dcfac94045b1ace20388460268b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30927330255508423, 'eval_wer': 0.2781181453747825, 'eval_runtime': 164.079, 'eval_samples_per_second': 4.437, 'eval_steps_per_second': 4.437, 'epoch': 27.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1443, 'learning_rate': 1.099476439790576e-05, 'epoch': 27.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09040edb86e4b2a96343ac717861d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3016777038574219, 'eval_wer': 0.2767566749867635, 'eval_runtime': 172.2781, 'eval_samples_per_second': 4.226, 'eval_steps_per_second': 4.226, 'epoch': 27.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1436, 'learning_rate': 8.37696335078534e-06, 'epoch': 28.34}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22f0062d2d24b42b0d522803ba02a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3099626898765564, 'eval_wer': 0.27683231223054233, 'eval_runtime': 166.2358, 'eval_samples_per_second': 4.379, 'eval_steps_per_second': 4.379, 'epoch': 28.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.14, 'learning_rate': 5.759162303664922e-06, 'epoch': 28.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0551eebd06748a4906eff69c4d00fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3092484176158905, 'eval_wer': 0.27607593979275397, 'eval_runtime': 179.6336, 'eval_samples_per_second': 4.053, 'eval_steps_per_second': 4.053, 'epoch': 28.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.141, 'learning_rate': 3.1413612565445026e-06, 'epoch': 29.37}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92686a33d3214325a56144e2d9241e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3074033856391907, 'eval_wer': 0.27456319491717723, 'eval_runtime': 167.2455, 'eval_samples_per_second': 4.353, 'eval_steps_per_second': 4.353, 'epoch': 29.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1358, 'learning_rate': 5.235602094240838e-07, 'epoch': 29.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3a595580b64abdb29863027ed71a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3078209161758423, 'eval_wer': 0.2758490280614174, 'eval_runtime': 170.7182, 'eval_samples_per_second': 4.264, 'eval_steps_per_second': 4.264, 'epoch': 29.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\Projects\\Speech to text-Urdu\\hugvenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 70994.548, 'train_samples_per_second': 1.23, 'train_steps_per_second': 0.041, 'train_loss': 1.0566150097502875, 'epoch': 29.99}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2910, training_loss=1.0566150097502875, metrics={'train_runtime': 70994.548, 'train_samples_per_second': 1.23, 'train_steps_per_second': 0.041, 'train_loss': 1.0566150097502875, 'epoch': 29.99})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the processor and the model to a local directory:\n",
    "\n",
    "This saves the processor and the model for future use. The model can be further fine-tuned on additional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./Model/final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on the test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reloading the model from the saved directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.from_pretrained(\"./Model/final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the transcriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTranscriptions(test_data):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(test_data[\"input_values\"]).unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    test_data[\"pred\"] = processor.batch_decode(pred_ids)[0]\n",
    "    test_data[\"gold\"] = processor.decode(test_data[\"labels\"], group_tokens=False)\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f82b1fd6c674d8c9bf38a0710c4fef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = ur_fine_dataset[\"test\"].map(testTranscriptions, remove_columns=ur_fine_dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Test WER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 0.225\n"
     ]
    }
   ],
   "source": [
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions = output[\"pred\"], references = output[\"gold\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the predicted and original transcriptions to text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToText(output):\n",
    "    with open(\"test_gold.txt\", \"w\", encoding = \"UTF-8\") as file:\n",
    "        for i in range(len(output[\"gold\"])):\n",
    "            file.write(output[\"gold\"][i] + \"\\n\")\n",
    "        file.close()\n",
    "    \n",
    "    with open(\"test_pred.txt\", \"w\", encoding = \"UTF-8\") as file:\n",
    "        for i in range(len(output[\"pred\"])):\n",
    "            file.write(output[\"pred\"][i] + \"\\n\")\n",
    "        file.close()\n",
    "\n",
    "\n",
    "saveToText(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
